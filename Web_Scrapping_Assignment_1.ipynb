{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0860bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Write a python program to display all the header tags from wikipedia.org\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen('https://en.wikipedia.org/wiki/Main_Page')\n",
    "bs = BeautifulSoup(html, \"html.parser\")\n",
    "titles = bs.find_all(['h1', 'h2','h3','h4','h5','h6'])\n",
    "print('List all the header tags :', *titles, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fdd82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release) and make data frame.\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import random\n",
    "url = 'http://www.imdb.com/chart/top100'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "def get_imd_movies(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    movies = soup.find_all(\"td\", class_=\"titleColumn\")\n",
    "    random.shuffle(movies)\n",
    "    return movies\n",
    "def get_imd_movie_info(movie):\n",
    "    movie_title = movie.a.contents[0]\n",
    "    movie_year = movie.span.contents[0]\n",
    "    movie_url = 'http://www.imdb.com' + movie.a['href']\n",
    "    return movie_title, movie_year, movie_url\n",
    "\n",
    "def imd_movie_picker():\n",
    "    ctr=0\n",
    "    print(\"--------------------------------------------\")\n",
    "    for movie in get_imd_movies('http://www.imdb.com/chart/top'):\n",
    "        movie_title, movie_year, movie_url = get_imd_movie_info(movie)\n",
    "        print(movie_title, movie_year)\n",
    "        print(\"--------------------------------------------\")\n",
    "        ctr=ctr+1\n",
    "        if (ctr==100):\n",
    "          break;   \n",
    "if __name__ == '__main__':\n",
    "    imd_movie_picker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c16ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Write a python program to display IMDB’s Top rated 100 indian movies’ data (i.e. name, rating, year of release) and make data frame.\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import random\n",
    "url = 'https://www.imdb.com/india/top-rated-indian-movies/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "def get_imd_movies(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    movies = soup.find_all(\"td\", class_=\"titleColumn\")\n",
    "    random.shuffle(movies)\n",
    "    return movies\n",
    "def get_imd_movie_info(movie):\n",
    "    movie_title = movie.a.contents[0]\n",
    "    movie_year = movie.span.contents[0]\n",
    "    movie_url = 'http://www.imdb.com' + movie.a['href']\n",
    "    return movie_title, movie_year, movie_url\n",
    "\n",
    "def imd_movie_picker():\n",
    "    ctr=0\n",
    "    print(\"--------------------------------------------\")\n",
    "    for movie in get_imd_movies('https://www.imdb.com/india/top-rated-indian-movies/'):\n",
    "        movie_title, movie_year, movie_url = get_imd_movie_info(movie)\n",
    "        print(movie_title, movie_year)\n",
    "        print(\"--------------------------------------------\")\n",
    "        ctr=ctr+1\n",
    "        if (ctr==100):\n",
    "          break;   \n",
    "if __name__ == '__main__':\n",
    "    imd_movie_picker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e008fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "##5) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "#a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "#b) Top 10 ODI Batsmen along with the records of their team and rating.\n",
    "#c) Top 10 ODI bowlers along with the records of their team and rating.\n",
    "#6) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "#a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "#b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "#c) Top 10 women’s ODI all-rounder along with the records of their team and rating.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "urls = [\"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\",\n",
    "\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\",\n",
    "\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\",\n",
    "\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\",\n",
    "\"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\",\n",
    "\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\",\n",
    "\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/bowling\",]\n",
    "final_result_file_name = \"All Ranking List.csv\"\n",
    "final_column_names = [\"Ranking Type\", \"Position\", \"Player Name\", \"Team Name\", \"Rating\", \"Career Best Rating\", \"Crawl URL\"]\n",
    "pd.DataFrame(columns=final_column_names).to_csv(final_result_file_name, sep=\"\\t\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "for url in urls:\n",
    "    request_object = requests.get(url, headers=headers)\n",
    "    html_content = request_object.text\n",
    "    print(request_object.status_code, \"->\", url)\n",
    "    soup_object = BeautifulSoup(html_content, \"lxml\")\n",
    "    for element in soup_object.select('[class=\"ranking-pos up\"], [class=\"ranking-pos down\"]'):\n",
    "        element.replace_with(BeautifulSoup(\"<\" + element.name + \"></\" + element.name + \">\", \"html.parser\"))\n",
    "\n",
    "    ranking_type = soup_object.select_one(\".rankings-block__title-container > h4\").text\n",
    "\n",
    "    result_file_name = ranking_type + \".csv\"\n",
    "    column_names = [\"Position\", \"Player Name\", \"Team Name\", \"Rating\", \"Career Best Rating\", \"Crawl URL\"]\n",
    "    pd.DataFrame(columns=column_names).to_csv(result_file_name, sep=\"\\t\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    for element in soup_object.select('table[class=\"table rankings-table\"] tr'):\n",
    "        if(element.find(\"th\")):\n",
    "            continue\n",
    "        data_dict = dict()\n",
    "        data_dict[\"Crawl URL\"] = url\n",
    "        data_dict[\"Ranking Type\"] = ranking_type\n",
    "        if(element.select_one('[class*=\"position\"]')):\n",
    "            data_dict[\"Position\"] = element.select_one('[class*=\"position\"]').text\n",
    "        for player_name in (element.select('a[href*=\"/player-rankings\"]')):\n",
    "            if(player_name.text.strip()):\n",
    "                data_dict[\"Player Name\"] = player_name.text\n",
    "        if(element.select_one('[class^=\"flag-15\"]')):\n",
    "            data_dict[\"Team Name\"] = element.select_one('[class^=\"flag-15\"]')[\"class\"][-1]\n",
    "        if(element.select_one('[class$=\"rating\"]')):\n",
    "            data_dict[\"Rating\"] = element.select_one('[class$=\"rating\"]').text\n",
    "        if(element.select_one('td.u-hide-phablet')):\n",
    "            data_dict[\"Career Best Rating\"] = element.select_one('td.u-hide-phablet').text\n",
    "        for key in data_dict.keys():\n",
    "            data_dict[key] = re.sub(r\"\\s+\", \" \", data_dict[key])\n",
    "            data_dict[key] = data_dict[key].strip()\n",
    "        pd.DataFrame([data_dict], columns=column_names).to_csv(result_file_name, sep=\"\\t\", index=False, header=False, encoding=\"utf-8\", mode=\"a\")\n",
    "        pd.DataFrame([data_dict], columns=final_column_names).to_csv(final_result_file_name, sep=\"\\t\", index=False, header=False, encoding=\"utf-8\", mode=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e8df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87596a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a python program to scrape mentioned details from dineout.co.in :\n",
    "#i) Restaurant name\n",
    "#ii) Cuisine\n",
    "#iii) Location\n",
    "#iv) Ratings\n",
    "#v) Image URL\n",
    "page=requests.get('https://www.dineout.co.in/delhi-restaurants/buffet-special')\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fa8ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(page.content)\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24af7df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_title=soup.find('div',class_=\"restnt-info cursor\")\n",
    "first_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b601be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=[]\n",
    "for i in soup.find_all('div',class_=\"restnt-info cursor\"):\n",
    "    titles.append(i.text)\n",
    "    \n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9024ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuisines=[]\n",
    "for i in soup.find_all('span',class_=\"double-line-ellipsis\"):\n",
    "    cuisines.append(i.text)\n",
    "    \n",
    "cuisines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d2182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "location=[]\n",
    "for i in soup.find_all('div',class_=\"restnt-loc ellipsis\"):\n",
    "    location.append(i.text)\n",
    "location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e1b9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings=[]\n",
    "for i in soup.find_all('div',class_=\"restnt-rating rating-4\"):\n",
    "    ratings.append(i.text)\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d890e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_url=[]\n",
    "for i in soup.find_all('img',class_=\"no-img\"):\n",
    "    images_url.append(i[\"data-src\"])\n",
    "images_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d4aae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({'Titles':titles,'Cuisines':cuisines,'Location':location,'Ratings':ratings,'Images_url':images_url})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529cb6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a python program to scrape first 10 product details which include product name , price , Image URL from https://www.bewakoof.com/women-tshirts?ga_q=tshirts .\n",
    "page=requests.get('https://www.bewakoof.com/women-tshirts?ga_q=tshirts')\n",
    "soup1=BeautifulSoup(page.content)\n",
    "productnames=[]\n",
    "for i in soup1('div',class_=\"productCardDetail \"):\n",
    "    productnames.append(i.text)\n",
    "prices=[]\n",
    "for i in soup1('span',class_=\"discountedPriceText \"):\n",
    "    prices.append(i.text)\n",
    "images_url=[]\n",
    "for i in soup1('img',class_=\"productImgTag\"):\n",
    "    images_url.append(i[\"src\"])\n",
    "import pandas as pd\n",
    "df1=pd.DataFrame({'Product_Names':productnames,'Prices':prices,'Images_url':images_url})\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce673c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a python program to scrape product name, price and discounts from https://meesho.com/bags-ladies/pl/p7vbp .\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "page=requests.get('https://meesho.com/bags-ladies/pl/p7vbp')\n",
    "soup2=BeautifulSoup(page.content)\n",
    "productnames1=[]\n",
    "for i in soup2('div',class_=\"Text__StyledText-sc-oo0kvp-0 bWSOET NewProductCard__ProductTitle_Desktop-sc-j0e7tu-4 cQhePS NewProductCard__ProductTitle_Desktop-sc-j0e7tu-4 cQhePS\"):\n",
    "    productnames1.append(i.text)\n",
    "prices1=[]\n",
    "for i in soup2('div',class_=\"Card__BaseCard-sc-b3n78k-0 iLPHgK NewProductCard__PriceRow-sc-j0e7tu-5 eyya-Dr NewProductCard__PriceRow-sc-j0e7tu-5 eyya-Dr\"):\n",
    "    prices1.append(i.text)\n",
    "discounts=[]\n",
    "for i in soup2('img',class_=\"Card__BaseCard-sc-b3n78k-0 dpAIfg NewProductCard__BadgeRow-sc-j0e7tu-13 ezFwfg NewProductCard__BadgeRow-sc-j0e7tu-13 ezFwfg\"):\n",
    "    discounts.append(i.text)\n",
    "df2=pd.DataFrame({'Product_Names':1,'Prices':prices1,'Discounts':discounts})\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c30a75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
